{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from freq_stacking_LFM_ACF_utils import *\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq, fftshift\n",
    "import scipy.signal\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent\n",
    "\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max RF center freqs in Hz\n",
    "fc_min = 500e6\n",
    "fc_max = 1.48e9\n",
    "\n",
    "delta_coeff = 0.01\n",
    "\n",
    "# min and max Bws in Hz\n",
    "Bmin = 10e6\n",
    "Bmax = 20e6\n",
    "delta_B = 1e6\n",
    "\n",
    "\n",
    "max_delay_time = 5e-6\n",
    "delay_time = 0.0\n",
    "\n",
    "# chirp rate\n",
    "chirp_rate = 50e6/10e-6\n",
    "\n",
    "\n",
    "num_subpulses = 50\n",
    "\n",
    "# maximum episode length\n",
    "max_episode_timesteps = 3000\n",
    "\n",
    "training_num = 200 # how many trajectories \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_baseband = Bmax  # baseband samp freq in Hz\n",
    "Fs_SWW_max = fc_max + Bmax / 2 - (fc_min - Bmax / 2)\n",
    "\n",
    "# time window size\n",
    "max_subpulse_duration = Bmax / chirp_rate\n",
    "time_window_size = 10 * max_subpulse_duration + max_delay_time\n",
    "\n",
    "\n",
    "# compute the state vector size\n",
    "LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "    BW_RF_array=np.array([20e6]),\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband\n",
    ")\n",
    "\n",
    "N_max = compute_Nup_f(LFM_rx_subpulses[0], Fs_baseband, Fs_SWW_max) # the state vector size\n",
    "\n",
    "# num_subpulses = int( Fs_SWW_max / Bmax * 1.2) # number of subpulses: may be a little bit larger than the none-overlapping case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACF_Env(Environment):\n",
    "\n",
    "    ####################################################################\n",
    "    # Required methods defs\n",
    "    ####################################################################\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_subpulses,\n",
    "        fc_min,\n",
    "        fc_max,\n",
    "        delta_coeff,\n",
    "        Bmin,\n",
    "        Bmax,\n",
    "        delta_B,\n",
    "        chirp_rate,\n",
    "        time_window_size,\n",
    "        Fs_baseband,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_subpulses = num_subpulses\n",
    "        self.fc_min = fc_min\n",
    "        self.fc_max = fc_max\n",
    "   \n",
    "        self.Bmin = Bmin\n",
    "        self.Bmax = Bmax\n",
    "        self.delta_B = delta_B\n",
    "\n",
    "        self.delta_coeff = delta_coeff\n",
    "    \n",
    "        self.chirp_rate = chirp_rate\n",
    "\n",
    "        self.time_window_size = time_window_size\n",
    "        self.Fs_baseband = Fs_baseband\n",
    "\n",
    "        self._current_best_sww_performance = 0 # the ISLR or PSL, i.e. the criteria of the waveform\n",
    "\n",
    "    def states(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(2, self.num_subpulses)\n",
    "        )  # the first row is the RF center freqs, and the second row is the BWs;\n",
    "        # the actions are normalized to [0, 1]\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(2, self.num_subpulses), min_value=-1, max_value=1\n",
    "        )  \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset state.\"\"\"\n",
    "        # state = np.random.random(size=(1,))\n",
    "        self.timestep = 0\n",
    "        self.current_state = np.zeros((2, self.num_subpulses))\n",
    "        # self.current_state[0] = signal.windows.hamming(self.num_subpulses)\n",
    "        self.current_state[0] = 1\n",
    "        self.current_state[1] = np.random.uniform(self.Bmin, self.Bmax)\n",
    "\n",
    "        return self.current_state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"[summary] Executes the given action(s) and advances the environment by one step.\n",
    "\n",
    "        The execute method implements the \"simulator\": how the environment reacts to an action\n",
    "            1. Increment timestamp\n",
    "            2. Update the current state: next_state <-- f(current_state, actions) (implement state transition)\n",
    "            3. Compute the reward accociated with the new state\n",
    "\n",
    "        returns state, terminal, reward\n",
    "        \"\"\"\n",
    "\n",
    "        # increment timestep\n",
    "        self.timestep += 1\n",
    "\n",
    "        # compute the next ACF and next reward\n",
    "        next_state, next_reward = self.compute_next_state_and_reward(actions)\n",
    "\n",
    "        # update the current ACF and reward\n",
    "        self.current_state = next_state\n",
    "        reward = next_reward\n",
    "\n",
    "        terminal = False  # maybe implement like this: if converge, terminal = True\n",
    "\n",
    "        return self.current_state, terminal, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Helper functions\n",
    "    ####################################################################\n",
    "\n",
    "    # helper function that implements the env model/simulator\n",
    "    def compute_next_state_and_reward(self, actions):\n",
    "        \"\"\"[summary] this method implements how env reacts to action:\n",
    "            action --> state\n",
    "\n",
    "            and also computes the reward associated with the state\n",
    "\n",
    "        Args:\n",
    "            actions ([type]): [description] the normalized actions\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] the the next_state and next_reward\n",
    "        \"\"\"\n",
    "        \n",
    "        next_state = np.zeros_like(actions)\n",
    "        next_state[0] = self.current_state[0] + self.delta_coeff * actions[0] # compute center freqs\n",
    "        next_state[1] = self.current_state[1] + self.delta_B * actions[1] # compute BWs\n",
    "\n",
    "        # make sure all center freqs and BWs are within range\n",
    "        next_state[0, next_state[0] > 1] = 1\n",
    "        next_state[0, next_state[0] < 0] = 0\n",
    "        next_state[1, next_state[1] > self.Bmax] = self.Bmax\n",
    "        next_state[1, next_state[1] < self.Bmin] = self.Bmin\n",
    "\n",
    "\n",
    "        # the following code computes the waveform performance associated with the next_state\n",
    "        fc_RF_freqs = self.fc_min + np.arange(self.num_subpulses) * self.Bmin  # the first row is the RF center freqs\n",
    "        BW_RF_array = next_state[1]  # the second row is the BWs\n",
    "        spectrum_weights = next_state[0]\n",
    "\n",
    "        # compute the time domain subpulses\n",
    "        LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "            BW_RF_array, self.chirp_rate, self.time_window_size, self.Fs_baseband\n",
    "        )\n",
    "\n",
    "        # multiply coeff\n",
    "        LFM_ref_subpulses =  np.reshape(next_state[0],(-1, 1)) * LFM_ref_subpulses\n",
    "\n",
    "        # filter BWs\n",
    "        Bs_array = BW_RF_array\n",
    "\n",
    "        # apply freq. stacking and get ACF\n",
    "        _, d_t = freq_stacking_v2(\n",
    "            LFM_rx_subpulses,\n",
    "            LFM_ref_subpulses,\n",
    "            fc_RF_freqs,\n",
    "            BW_RF_array,\n",
    "            Bs_array,\n",
    "            self.Fs_baseband,\n",
    "            spectrum_weights\n",
    "        )\n",
    "\n",
    "        # compute ACF\n",
    "        ACF = np.abs(d_t) / np.max(np.abs(d_t))\n",
    "\n",
    "        # compute the sww_performance associated with this ACF\n",
    "        sww_performance = -int_sidelobe_ratio(ACF)\n",
    "        reward =  sww_performance - self._current_best_sww_performance\n",
    "\n",
    "        # update the max score\n",
    "        if sww_performance > self._current_best_sww_performance:\n",
    "            self._current_best_sww_performance = sww_performance\n",
    "            self._current_best_state = next_state\n",
    "            print(f\"current_best_performance = {self._current_best_sww_performance}, time step = {self.timestep}\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Optional methods defs\n",
    "    ####################################################################\n",
    "\n",
    "    # Optional, should only be defined if environment has a natural maximum\n",
    "    # episode length\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional\n",
    "    def close(self):\n",
    "        super().close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ACF_env\n",
    "\n",
    "\n",
    "my_ACF_Env = Environment.create(\n",
    "    environment=ACF_Env,\n",
    "    max_episode_timesteps=max_episode_timesteps,\n",
    "    num_subpulses = num_subpulses,\n",
    "    fc_min=fc_min,\n",
    "    fc_max=fc_max,\n",
    "    delta_coeff = delta_coeff,\n",
    "    Bmin=Bmin,\n",
    "    Bmax=Bmax,\n",
    "    delta_B = delta_B,\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup\n",
    "\n",
    " Here we configure a type of agent to learn against this environment. There are many agent configurations to choose from, which we will not cover here. We will not discuss what type of agent to choose here -- we will just take a basic agent to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=my_ACF_Env, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Untrained Agent Performance\n",
    "The agent just initializes a policy and use that policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration\n",
      "current_best_performance = 27.198544898937712, time step = 1\n",
      "current_best_performance = 27.66818677942945, time step = 2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "  : Tensor had NaN values\n\t [[node agent/VerifyFinite_1/CheckNumerics (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensor_spec.py:319) ]] [Op:__inference_act_1085]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node agent/VerifyFinite_1/CheckNumerics:\n agent/assert_equal_5/Assert/AssertGuard (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensor_spec.py:312)\t\n agent/StatefulPartitionedCall (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/module.py:136)\n\nFunction call stack:\nact\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-876aa3d8faae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ACF_Env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states, internals, parallel, independent, deterministic, evaluation)\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         return super().act(\n\u001b[0m\u001b[1;32m    416\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindependent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindependent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/recorder.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states, internals, parallel, independent, deterministic, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# fn_act()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_agent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             actions, internals = self.fn_act(\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindependent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindependent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_internals_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_internals_none\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mfn_act\u001b[0;34m(self, states, internals, parallel, independent, deterministic, is_internals_none, num_parallel)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Model.act()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindependent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             actions, timesteps = self.model.act(\n\u001b[0m\u001b[1;32m    463\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauxiliaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorforce/core/module.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, _initialize, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Apply function graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0moutput_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgraph_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loop_body\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 return output_signature.args_to_kwargs(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:   : Tensor had NaN values\n\t [[node agent/VerifyFinite_1/CheckNumerics (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensor_spec.py:319) ]] [Op:__inference_act_1085]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node agent/VerifyFinite_1/CheckNumerics:\n agent/assert_equal_5/Assert/AssertGuard (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/utils/tensor_spec.py:312)\t\n agent/StatefulPartitionedCall (defined at /home/cs229/anaconda3/lib/python3.8/site-packages/tensorforce/core/module.py:136)\n\nFunction call stack:\nact\n"
     ]
    }
   ],
   "source": [
    "# Train for 200 episodes\n",
    "\n",
    "for _ in range(training_num):\n",
    "    states = my_ACF_Env.reset()\n",
    "    terminal = False\n",
    "    print(f\"{_} iteration\")\n",
    "    \n",
    "    while not terminal:\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = my_ACF_Env.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "      \n",
    "        #print(f\"time = {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.691567451915905"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ACF_Env._current_best_sww_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0e+00, 1.0e+00, 9.9e-01, 1.0e+00, 1.0e+00, 1.0e+00, 9.9e-01,\n",
       "        1.0e+00, 9.9e-01, 9.9e-01, 9.9e-01, 1.0e+00, 9.9e-01, 9.9e-01,\n",
       "        9.9e-01, 9.9e-01, 1.0e+00, 1.0e+00, 9.9e-01, 1.0e+00, 1.0e+00,\n",
       "        1.0e+00, 1.0e+00, 9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01,\n",
       "        9.9e-01, 9.9e-01, 9.9e-01, 1.0e+00, 1.0e+00, 9.9e-01, 9.9e-01,\n",
       "        1.0e+00, 1.0e+00, 1.0e+00, 9.9e-01, 1.0e+00, 9.9e-01, 9.9e-01,\n",
       "        9.9e-01, 9.9e-01, 9.9e-01, 1.0e+00, 1.0e+00, 1.0e+00, 9.9e-01,\n",
       "        1.0e+00],\n",
       "       [1.9e+07, 2.0e+07, 1.9e+07, 2.0e+07, 1.9e+07, 2.0e+07, 2.0e+07,\n",
       "        1.9e+07, 2.0e+07, 2.0e+07, 2.0e+07, 1.9e+07, 2.0e+07, 1.9e+07,\n",
       "        2.0e+07, 2.0e+07, 2.0e+07, 2.0e+07, 1.9e+07, 1.9e+07, 1.9e+07,\n",
       "        2.0e+07, 1.9e+07, 1.9e+07, 2.0e+07, 2.0e+07, 2.0e+07, 2.0e+07,\n",
       "        1.9e+07, 1.9e+07, 2.0e+07, 1.9e+07, 2.0e+07, 2.0e+07, 2.0e+07,\n",
       "        1.9e+07, 1.9e+07, 2.0e+07, 2.0e+07, 1.9e+07, 2.0e+07, 1.9e+07,\n",
       "        1.9e+07, 1.9e+07, 2.0e+07, 2.0e+07, 1.9e+07, 1.9e+07, 1.9e+07,\n",
       "        1.9e+07]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ACF_Env._current_best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-ed15f2df15e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_list' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(reward_list)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subpulses, np.max(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3239e8400f967b912aa5ff52c1d70244e44ae8ab469e58fe9dd95ca3971341f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
