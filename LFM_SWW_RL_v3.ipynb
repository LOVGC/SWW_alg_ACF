{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from freq_stacking_LFM_ACF_utils import *\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq, fftshift\n",
    "import scipy.signal\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max RF center freqs in Hz\n",
    "fc_min = 500e6\n",
    "fc_max = 1.48e9\n",
    "\n",
    "delta_coeff = 0.01\n",
    "\n",
    "# min and max Bws in Hz\n",
    "Bmin = 10e6\n",
    "Bmax = 20e6\n",
    "delta_B = 1e6\n",
    "\n",
    "\n",
    "max_delay_time = 5e-6\n",
    "delay_time = 0.0\n",
    "\n",
    "# chirp rate\n",
    "chirp_rate = 50e6/10e-6\n",
    "\n",
    "\n",
    "num_subpulses = 50\n",
    "\n",
    "# maximum episode length\n",
    "max_episode_timesteps = 3000\n",
    "\n",
    "training_num = 200 # how many trajectories \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_baseband = Bmax  # baseband samp freq in Hz\n",
    "Fs_SWW_max = fc_max + Bmax / 2 - (fc_min - Bmax / 2)\n",
    "\n",
    "# time window size\n",
    "max_subpulse_duration = Bmax / chirp_rate\n",
    "time_window_size = 10 * max_subpulse_duration + max_delay_time\n",
    "\n",
    "\n",
    "# compute the state vector size\n",
    "LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "    BW_RF_array=np.array([20e6]),\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband\n",
    ")\n",
    "\n",
    "N_max = compute_Nup_f(LFM_rx_subpulses[0], Fs_baseband, Fs_SWW_max) # the state vector size\n",
    "\n",
    "# num_subpulses = int( Fs_SWW_max / Bmax * 1.2) # number of subpulses: may be a little bit larger than the none-overlapping case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACF_Env(Environment):\n",
    "\n",
    "    ####################################################################\n",
    "    # Required methods defs\n",
    "    ####################################################################\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_subpulses,\n",
    "        fc_min,\n",
    "        fc_max,\n",
    "        delta_coeff,\n",
    "        Bmin,\n",
    "        Bmax,\n",
    "        delta_B,\n",
    "        chirp_rate,\n",
    "        time_window_size,\n",
    "        Fs_baseband,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_subpulses = num_subpulses\n",
    "        self.fc_min = fc_min\n",
    "        self.fc_max = fc_max\n",
    "   \n",
    "        self.Bmin = Bmin\n",
    "        self.Bmax = Bmax\n",
    "        self.delta_B = delta_B\n",
    "\n",
    "        self.delta_coeff = delta_coeff\n",
    "    \n",
    "        self.chirp_rate = chirp_rate\n",
    "\n",
    "        self.time_window_size = time_window_size\n",
    "        self.Fs_baseband = Fs_baseband\n",
    "\n",
    "        self._current_best_sww_performance = 0 # the ISLR or PSL, i.e. the criteria of the waveform\n",
    "\n",
    "    def states(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(2, self.num_subpulses)\n",
    "        )  # the first row is the RF center freqs, and the second row is the BWs;\n",
    "        # the actions are normalized to [0, 1]\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(2, self.num_subpulses), min_value=-1, max_value=1\n",
    "        )  \n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset state.\"\"\"\n",
    "        # state = np.random.random(size=(1,))\n",
    "        self.timestep = 0\n",
    "        self.current_state = np.zeros((2, self.num_subpulses))\n",
    "        self.current_state[0] = 1\n",
    "        self.current_state[1] =self.Bmax\n",
    "\n",
    "        return self.current_state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"[summary] Executes the given action(s) and advances the environment by one step.\n",
    "\n",
    "        The execute method implements the \"simulator\": how the environment reacts to an action\n",
    "            1. Increment timestamp\n",
    "            2. Update the current state: next_state <-- f(current_state, actions) (implement state transition)\n",
    "            3. Compute the reward accociated with the new state\n",
    "\n",
    "        returns state, terminal, reward\n",
    "        \"\"\"\n",
    "\n",
    "        # increment timestep\n",
    "        self.timestep += 1\n",
    "\n",
    "        # compute the next ACF and next reward\n",
    "        next_state, next_reward = self.compute_next_state_and_reward(actions)\n",
    "\n",
    "        # update the current ACF and reward\n",
    "        self.current_state = next_state\n",
    "        reward = next_reward\n",
    "\n",
    "        terminal = False  # maybe implement like this: if converge, terminal = True\n",
    "\n",
    "        return self.current_state, terminal, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Helper functions\n",
    "    ####################################################################\n",
    "\n",
    "    # helper function that implements the env model/simulator\n",
    "    def compute_next_state_and_reward(self, actions):\n",
    "        \"\"\"[summary] this method implements how env reacts to action:\n",
    "            action --> state\n",
    "\n",
    "            and also computes the reward associated with the state\n",
    "\n",
    "        Args:\n",
    "            actions ([type]): [description] the normalized actions\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] the the next_state and next_reward\n",
    "        \"\"\"\n",
    "        \n",
    "        next_state = np.zeros_like(actions)\n",
    "        next_state[0] = self.current_state[0] + self.delta_coeff * actions[0] # compute center freqs\n",
    "        next_state[1] = self.current_state[1] + self.delta_B * actions[1] # compute BWs\n",
    "\n",
    "        # make sure all center freqs and BWs are within range\n",
    "        next_state[0, next_state[0] > 1] = 1\n",
    "        next_state[0, next_state[0] < 0] = 0\n",
    "        next_state[1, next_state[1] > self.Bmax] = self.Bmax\n",
    "        next_state[1, next_state[1] < self.Bmin] = self.Bmin\n",
    "\n",
    "\n",
    "        # the following code computes the waveform performance associated with the next_state\n",
    "        fc_RF_freqs = self.fc_min + np.arange(self.num_subpulses) * self.Bmin  # the first row is the RF center freqs\n",
    "        BW_RF_array = next_state[1]  # the second row is the BWs\n",
    "        \n",
    "\n",
    "        # compute the time domain subpulses\n",
    "        LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "            BW_RF_array, self.chirp_rate, self.time_window_size, self.Fs_baseband\n",
    "        )\n",
    "\n",
    "        # multiply coeff\n",
    "        LFM_ref_subpulses =  np.reshape(next_state[0],(-1, 1)) * LFM_ref_subpulses\n",
    "\n",
    "        # filter BWs\n",
    "        Bs_array = BW_RF_array\n",
    "\n",
    "        # apply freq. stacking and get ACF\n",
    "        _, d_t = freq_stacking(\n",
    "            LFM_rx_subpulses,\n",
    "            LFM_ref_subpulses,\n",
    "            fc_RF_freqs,\n",
    "            BW_RF_array,\n",
    "            Bs_array,\n",
    "            self.Fs_baseband,\n",
    "        )\n",
    "\n",
    "        # compute ACF\n",
    "        ACF = np.abs(d_t) / np.max(np.abs(d_t))\n",
    "\n",
    "        # compute the sww_performance associated with this ACF\n",
    "        sww_performance = -int_sidelobe_ratio(ACF)\n",
    "        reward =  sww_performance - self._current_best_sww_performance\n",
    "\n",
    "        # update the max score\n",
    "        if sww_performance > self._current_best_sww_performance:\n",
    "            self._current_best_sww_performance = sww_performance\n",
    "            self._current_best_state = next_state\n",
    "            print(f\"current_best_performance = {self._current_best_sww_performance}\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Optional methods defs\n",
    "    ####################################################################\n",
    "\n",
    "    # Optional, should only be defined if environment has a natural maximum\n",
    "    # episode length\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional\n",
    "    def close(self):\n",
    "        super().close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ACF_env\n",
    "\n",
    "\n",
    "my_ACF_Env = Environment.create(\n",
    "    environment=ACF_Env,\n",
    "    max_episode_timesteps=max_episode_timesteps,\n",
    "    num_subpulses = num_subpulses,\n",
    "    fc_min=fc_min,\n",
    "    fc_max=fc_max,\n",
    "    delta_coeff = delta_coeff,\n",
    "    Bmin=Bmin,\n",
    "    Bmax=Bmax,\n",
    "    delta_B = delta_B,\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup\n",
    "\n",
    " Here we configure a type of agent to learn against this environment. There are many agent configurations to choose from, which we will not cover here. We will not discuss what type of agent to choose here -- we will just take a basic agent to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=my_ACF_Env, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-6),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Untrained Agent Performance\n",
    "The agent just initializes a policy and use that policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration\n",
      "current_best_performance = 28.651874226085383 1\n"
     ]
    }
   ],
   "source": [
    "# Train for 200 episodes\n",
    "\n",
    "for _ in range(training_num):\n",
    "    states = my_ACF_Env.reset()\n",
    "    terminal = False\n",
    "    print(f\"{_} iteration\")\n",
    "    \n",
    "    while not terminal:\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = my_ACF_Env.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "      \n",
    "        #print(f\"time = {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.723087659848794"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ACF_Env._current_best_sww_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9e-01, 1.0e+00, 9.9e-01, 9.9e-01, 9.9e-01, 1.0e+00, 9.9e-01,\n",
       "        1.0e+00, 1.0e+00, 9.9e-01, 1.0e+00, 1.0e+00, 9.9e-01, 1.0e+00,\n",
       "        9.9e-01, 1.0e+00, 1.0e+00, 1.0e+00, 1.0e+00, 1.0e+00, 9.9e-01,\n",
       "        9.9e-01, 9.9e-01, 1.0e+00, 9.9e-01, 1.0e+00, 9.9e-01, 9.9e-01,\n",
       "        9.9e-01, 1.0e+00, 1.0e+00, 1.0e+00, 1.0e+00, 9.9e-01, 9.9e-01,\n",
       "        1.0e+00, 9.9e-01, 9.9e-01, 1.0e+00, 9.9e-01, 9.9e-01, 1.0e+00,\n",
       "        9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01, 9.9e-01,\n",
       "        1.0e+00],\n",
       "       [2.0e+07, 2.0e+07, 1.9e+07, 2.0e+07, 2.0e+07, 1.9e+07, 1.9e+07,\n",
       "        1.9e+07, 1.9e+07, 2.0e+07, 1.9e+07, 1.9e+07, 2.0e+07, 2.0e+07,\n",
       "        1.9e+07, 2.0e+07, 1.9e+07, 1.9e+07, 2.0e+07, 2.0e+07, 1.9e+07,\n",
       "        2.0e+07, 1.9e+07, 2.0e+07, 2.0e+07, 2.0e+07, 1.9e+07, 1.9e+07,\n",
       "        1.9e+07, 2.0e+07, 1.9e+07, 1.9e+07, 1.9e+07, 1.9e+07, 2.0e+07,\n",
       "        2.0e+07, 2.0e+07, 1.9e+07, 2.0e+07, 1.9e+07, 2.0e+07, 2.0e+07,\n",
       "        2.0e+07, 1.9e+07, 1.9e+07, 1.9e+07, 1.9e+07, 1.9e+07, 2.0e+07,\n",
       "        1.9e+07]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ACF_Env._current_best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-ed15f2df15e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_list' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(reward_list)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subpulses, np.max(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3239e8400f967b912aa5ff52c1d70244e44ae8ab469e58fe9dd95ca3971341f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
