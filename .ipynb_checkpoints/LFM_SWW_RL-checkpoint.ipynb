{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from freq_stacking_LFM_ACF_utils import *\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq, fftshift\n",
    "import scipy.signal\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.agents import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max RF center freqs in Hz\n",
    "fc_min = 500e6\n",
    "fc_max = 1.48e9\n",
    "\n",
    "# min and max Bws in Hz\n",
    "Bmin = 18e6\n",
    "Bmax = 20e6\n",
    "\n",
    "\n",
    "max_delay_time = 5e-6\n",
    "delay_time = 0.0\n",
    "\n",
    "# chirp rate\n",
    "chirp_rate = 50e6/10e-6\n",
    "\n",
    "\n",
    "num_subpulses = 50\n",
    "\n",
    "# maximum episode length\n",
    "max_episode_timesteps = 6000\n",
    "\n",
    "training_num = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs_baseband = Bmax  # baseband samp freq in Hz\n",
    "Fs_SWW_max = fc_max + Bmax / 2 - (fc_min - Bmax / 2)\n",
    "\n",
    "# time window size\n",
    "max_subpulse_duration = Bmax / chirp_rate\n",
    "time_window_size = 10 * max_subpulse_duration + max_delay_time\n",
    "\n",
    "\n",
    "# compute the state vector size\n",
    "LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "    BW_RF_array=np.array([20e6]),\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband\n",
    ")\n",
    "\n",
    "N_max = compute_Nup_f(LFM_rx_subpulses[0], Fs_baseband, Fs_SWW_max) # the state vector size\n",
    "\n",
    "# num_subpulses = int( Fs_SWW_max / Bmax * 1.2) # number of subpulses: may be a little bit larger than the none-overlapping case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radar Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACF_Env(Environment):\n",
    "\n",
    "    ####################################################################\n",
    "    # Required methods defs\n",
    "    ####################################################################\n",
    "    def __init__(\n",
    "        self,\n",
    "        N_max,\n",
    "        num_subpulses,\n",
    "        fc_min,\n",
    "        fc_max,\n",
    "        Bmin,\n",
    "        Bmax,\n",
    "        chirp_rate,\n",
    "        time_window_size,\n",
    "        Fs_baseband,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.N_max = N_max  # size of the state vector\n",
    "        self.num_subpulses = num_subpulses\n",
    "        self.fc_min = fc_min\n",
    "        self.fc_max = fc_max\n",
    "        self.Bmin = Bmin\n",
    "        self.Bmax = Bmax\n",
    "        self.chirp_rate = chirp_rate\n",
    "\n",
    "        self.time_window_size = time_window_size\n",
    "        self.Fs_baseband = Fs_baseband\n",
    "\n",
    "    def states(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(self.N_max,), min_value=0\n",
    "        )  # min/max state values are optional\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "            type=\"float\", shape=(2, self.num_subpulses), min_value=0, max_value=1\n",
    "        )  # the first row is the RF center freqs, and the second row is the BWs;\n",
    "        # the actions are normalized to [0, 1]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset state.\"\"\"\n",
    "        # state = np.random.random(size=(1,))\n",
    "        self.timestep = 0\n",
    "        self.current_ACF = np.abs(np.random.randn(self.N_max))\n",
    "\n",
    "        self.current_max_previouse_score = 0 # the ISLR or PSL, i.e. the criteria of the waveform\n",
    "\n",
    "        return self.current_ACF\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"[summary] Executes the given action(s) and advances the environment by one step.\n",
    "\n",
    "        The execute method implements the \"simulator\": how the environment reacts to an action\n",
    "            1. Increment timestamp\n",
    "            2. Update the current state: next_state <-- f(current_state, actions) (implement state transition)\n",
    "            3. Compute the reward accociated with the new state\n",
    "\n",
    "        returns state, terminal, reward\n",
    "        \"\"\"\n",
    "\n",
    "        # increment timestep\n",
    "        self.timestep += 1\n",
    "\n",
    "        # compute the next ACF and next reward\n",
    "        next_ACF, next_reward = self.compute_ACF_and_reward(actions)\n",
    "\n",
    "        # update the current ACF and reward\n",
    "        self.current_ACF[:] = np.concatenate(\n",
    "            (next_ACF, np.zeros(int(self.N_max - next_ACF.size)) )\n",
    "        )\n",
    "        reward = next_reward\n",
    "\n",
    "        terminal = False  # maybe implement like this: if converge, terminal = True\n",
    "\n",
    "        return self.current_ACF, terminal, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Helper functions\n",
    "    ####################################################################\n",
    "\n",
    "    def normalized_action_to_real_action(self, normalized_action):\n",
    "        real_action = np.zeros_like(normalized_action)\n",
    "        real_action[0] = (\n",
    "            self.fc_min + (self.fc_max - self.fc_min) * normalized_action[0]\n",
    "        )  # convert center freqs\n",
    "        real_action[1] = (\n",
    "            self.Bmin + (self.Bmax - self.Bmin) * normalized_action[1]\n",
    "        )  # convert BWs\n",
    "        return real_action\n",
    "        \n",
    "    def compute_ACF_and_reward(self, actions):\n",
    "        \"\"\"[summary] this method implements how state reacts to action:\n",
    "            action --> state\n",
    "\n",
    "            and also computes the reward associated with the state\n",
    "\n",
    "        Args:\n",
    "            actions ([type]): [description] the normalized actions\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description] the normalized ACF in db\n",
    "        \"\"\"\n",
    "        real_actions = self.normalized_action_to_real_action(actions)\n",
    "        fc_RF_freqs = real_actions[0]  # the first row is the RF center freqs\n",
    "        BW_RF_array = real_actions[1]  # the second row is the BWs\n",
    "        \n",
    "        # _, BW_RF_sww = compute_para_for_RF_sww(fc_RF_freqs, BW_RF_array)\n",
    "\n",
    "        # compute the time domain subpulses\n",
    "        LFM_rx_subpulses, LFM_ref_subpulses = generate_LFM_rx_ref_subpulses_for_ACF(\n",
    "            BW_RF_array, self.chirp_rate, self.time_window_size, self.Fs_baseband\n",
    "        )\n",
    "\n",
    "        # filter BWs\n",
    "        Bs_array = BW_RF_array\n",
    "\n",
    "        # apply freq. stacking and get ACF\n",
    "        _, d_t = freq_stacking(\n",
    "            LFM_rx_subpulses,\n",
    "            LFM_ref_subpulses,\n",
    "            fc_RF_freqs,\n",
    "            BW_RF_array,\n",
    "            Bs_array,\n",
    "            self.Fs_baseband,\n",
    "        )\n",
    "\n",
    "        # compute ACF\n",
    "        ACF = np.abs(d_t)\n",
    "\n",
    "        # compute the reward associated with this ACF\n",
    "        score = -int_sidelobe_ratio(ACF)\n",
    "        reward =  score - self.current_max_previouse_score\n",
    "\n",
    "        # update the max score\n",
    "        if score > self.current_max_previouse_score:\n",
    "            self.current_max_previouse_score = score\n",
    "        \n",
    "        if score > 22.9:\n",
    "            print(f\"-ISLR = {score}\")\n",
    "\n",
    "        return ACF, reward\n",
    "\n",
    "    ####################################################################\n",
    "    # Optional methods defs\n",
    "    ####################################################################\n",
    "\n",
    "    # Optional, should only be defined if environment has a natural maximum\n",
    "    # episode length\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional\n",
    "    def close(self):\n",
    "        super().close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ACF_env\n",
    "\n",
    "\n",
    "my_ACF_Env = Environment.create(\n",
    "    environment=ACF_Env,\n",
    "    max_episode_timesteps=max_episode_timesteps,\n",
    "    N_max=N_max,\n",
    "    num_subpulses = num_subpulses,\n",
    "    fc_min=fc_min,\n",
    "    fc_max=fc_max,\n",
    "    Bmin=Bmin,\n",
    "    Bmax=Bmax,\n",
    "    chirp_rate=chirp_rate,\n",
    "    time_window_size=time_window_size,\n",
    "    Fs_baseband=Fs_baseband,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Setup\n",
    "\n",
    " Here we configure a type of agent to learn against this environment. There are many agent configurations to choose from, which we will not cover here. We will not discuss what type of agent to choose here -- we will just take a basic agent to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No max_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent.create(\n",
    "    agent='tensorforce', environment=my_ACF_Env, update=64,\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: Untrained Agent Performance\n",
    "The agent just initializes a policy and use that policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration\n"
     ]
    }
   ],
   "source": [
    "# Train for 200 episodes\n",
    "states_list = []\n",
    "actions_list = []\n",
    "reward_list = []\n",
    "for _ in range(training_num):\n",
    "    states = my_ACF_Env.reset()\n",
    "    terminal = False\n",
    "    print(f\"{_} iteration\")\n",
    "    time = 0\n",
    "    while not terminal:\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = my_ACF_Env.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)\n",
    "\n",
    "        states_list.append(states)\n",
    "        actions_list.append(actions)\n",
    "        reward_list.append(reward)\n",
    "        time += 1\n",
    "        #print(f\"time = {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(reward_list)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subpulses, np.max(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3239e8400f967b912aa5ff52c1d70244e44ae8ab469e58fe9dd95ca3971341f4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
